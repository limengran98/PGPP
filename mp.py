import warnings
warnings.filterwarnings("ignore")

import argparse
import io
import json
import os
import os.path as osp
import pickle
import time
import csv

import torch
import torch.nn.functional as F
from torch import optim
import numpy as np
import pandas as pd

from data import load_data, is_continuous, is_large, to_edge_tensor
from utils import *
from model import arbLabel, arbLoss
from metric import homophil_index
from eva import cluster_acc

from torch_geometric.data import Data
from torch_geometric.nn import GCNConv
from torch_geometric.nn.inits import uniform
from torch_geometric.utils import subgraph, to_dense_adj, dense_to_sparse
from torch_geometric.nn import DenseGCNConv


from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
from sklearn.metrics.cluster import normalized_mutual_info_score as nmi_score
from sklearn.metrics import adjusted_rand_score as ari_score
from sklearn.model_selection import KFold
from sklearn.utils import shuffle
from sklearn.metrics import accuracy_score,precision_score,recall_score,classification_report,f1_score,normalized_mutual_info_score, adjusted_rand_score

from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier
import xgboost as xgb
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score
from sklearn.metrics import mean_squared_error, mean_absolute_error
from torch.optim.lr_scheduler import StepLR
import numpy as np
from sklearn.semi_supervised import LabelPropagation
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering
from sklearn.mixture import GaussianMixture

def parse_args():
    """
    Parse command line arguments.
    """
    parser = argparse.ArgumentParser()
    parser.add_argument('--data', type=str, default='Roman-empire') # ["Roman-empire", "Amazon-ratings", "Minesweeper", "Tolokers",'squirrel']
    parser.add_argument('--y-ratio', type=float, default=0.0)
    parser.add_argument('--seed', type=int, default=0)
    parser.add_argument('--gpu', type=int, default=0)

    parser.add_argument('--missrate', type=float, default=0.6) 
    parser.add_argument('--alpha', type=float, default=1) # 1
    parser.add_argument('--beta', type=float, default=0) # 0
    parser.add_argument('--gamma', type=float, default=0.9) # 0.9
    parser.add_argument('--num_iter', type=int, default=2)

    parser.add_argument('--dropout', type=float, default=0.2)
    parser.add_argument('--lr', type=float, default=1e-3) #5e-2
    parser.add_argument('--layers', type=int, default=2)
    parser.add_argument('--hidden_size', type=int, default=256)
    parser.add_argument('--epochs', type=int, default=1000)
    parser.add_argument('--patience', type=int, default=0)
    parser.add_argument('--model', type=str, default='MLP') 
    parser.add_argument('--MPNN', type=str, default='GCN') 

    return parser.parse_args()

def random_mask_tensor(X, mask_ratio, seed=None):
    torch.manual_seed(seed)
    rows, cols = X.size()
    num_zeros = int(mask_ratio * rows * cols)
    zero_indices = torch.randperm(rows * cols)[:num_zeros]
    masked_X = X.clone()
    masked_X.view(-1)[zero_indices] = 0
    return masked_X

def get_best_pseudo_labels(x_missing_filled, y_all, num_classes):
    """
    :param x_missing_filled: Node features with missing attributes filled
    :param y_all: Labels for all nodes
    :param num_classes: Number of clusters
    :return: Pseudo-labels generated by the clustering method with the best performance
    """
    clustering_methods = {
        'KMeans': KMeans(n_clusters=num_classes),
        'Agglomerative': AgglomerativeClustering(n_clusters=num_classes),
        # 'Spectral': SpectralClustering(n_clusters=num_classes, affinity='nearest_neighbors'),
        # 'GMM': GaussianMixture(n_components=num_classes)
    }

    best_nmi = -1
    best_ac = -1
    best_ari = -1
    best_f1 = -1
    best_labels = None
    best_method = None

    for method_name, method in clustering_methods.items():
        if method_name == 'GMM':
            x_data = x_missing_filled.cpu().numpy()
            y_pred = method.fit_predict(x_data)
        else:
            y_pred = method.fit_predict(x_missing_filled.cpu().numpy())

        y_pred = torch.tensor(y_pred, dtype=torch.long)

        y_true = y_all.cpu().numpy()
        y_pred_np = y_pred.cpu().numpy()

        nmi = normalized_mutual_info_score(y_true, y_pred_np)
        ac, f1_ = cluster_acc(y_true, y_pred_np)
        ari = adjusted_rand_score(y_true, y_pred_np)


        if ac > best_ac:
            best_nmi = nmi
            best_ac = ac
            best_ari = ari
            best_f1 = f1_
            best_labels = y_pred
            best_method = method_name

    print(f"Best Method: {best_method}")
    print(f'AC: {best_ac:.4f}, NMI: {best_nmi:.4f}, ARI: {best_ari:.4f}, F1: {best_f1:.4f}')
    
    return best_labels
"""
Main function.
"""
def main():
    args = parse_args()
    print(args)
    device = to_device(args.gpu)


    data, trn_nodes, test_nodes = load_data(args, split=(1-args.missrate, args.missrate), seed=args.seed)
    y_all = data.y

    x_all = data.x.clone()
    x_missing = x_all.clone()
    x_missing[test_nodes] = 0

    propagation = APA(data.edge_index, x_all, trn_nodes)
    x_missing_filled = propagation.fp(x_missing).cuda()

    
    #Clustering
    # num_classes = (y_all.max() + 1).item()
    # kmeans = KMeans(n_clusters=num_classes)
    # y_pred = kmeans.fit_predict(x_missing_filled.cpu().numpy())
    # y_pred = torch.Tensor(y_pred).long()  
    # y_true = y_all.cpu().numpy()
    # y_pred_np = y_pred.cpu().numpy()
    # nmi = normalized_mutual_info_score(y_true, y_pred_np)
    # ac, f1_ = cluster_acc(y_true, y_pred_np)
    # ari = adjusted_rand_score(y_true, y_pred_np)
    # print(f'AC: {ac:.4f}, '
    #     f'NMI: {nmi:.4f}, '
    #     f'ARI: {ari:.4f}, '
    #     f'F1: {f1_:.4f}')
    num_classes = (y_all.max() + 1).item()
    pseudo_labels = get_best_pseudo_labels(x_missing_filled, y_all, num_classes)


    propagation_model = arbLabel(data.edge_index, x_all, pseudo_labels, trn_nodes)
    x_feature = propagation_model.arb_label_100(x_missing, alpha = args.alpha, beta = args.beta, gamma = args.gamma, num_iter = args.num_iter).cuda()

    # propagation = APA(data.edge_index, x_all, trn_nodes)
    # x_feature = propagation.fp(x_missing).cuda()


    dataset_name = args.data
    # Create the output directory if it doesn't exist
    output_dir = os.path.join('output', dataset_name)
    os.makedirs(output_dir, exist_ok=True)
    # Save the variables to the specified path
    torch.save(x_feature, os.path.join(output_dir, 'ox_feature.pt'))
    torch.save(trn_nodes, os.path.join(output_dir, 'otrn_nodes.pt'))
    torch.save(test_nodes, os.path.join(output_dir, 'otest_nodes.pt'))
    torch.save(y_all, os.path.join(output_dir, 'oy_all.pt'))



    print(x_feature.shape)

    x_all = x_all.to(device)
    x_feature = x_feature.to(device)
    edge_index = data.edge_index.to(device)
    edge_weight = data.edge_weight
    edge_nums = data.num_edges
    num_nodes = x_feature.size(0)
    num_features = x_feature.size(1)
    num_classes = (data.y.max() + 1).item()
    

    #Re
    scores = []
    mse = mean_squared_error(x_feature[test_nodes].cpu(), x_all[test_nodes].cpu())
    rmse = np.sqrt(mse)  
    mae = mean_absolute_error(x_feature[test_nodes].cpu(), x_all[test_nodes].cpu())
    scores.append(mse)
    scores.append(rmse)
    scores.append(mae)
    Restructure = scores
    print(f'Mean Squared Error (MSE): {mse:.4f}')
    print(f'Root Mean Squared Error (RMSE): {rmse:.4f}')
    print(f'Mean Absolute Error (MAE): {mae:.4f}')


    # Class
    test_accuracies = []
    test_f1s = []
    test_roc_aucs = []
    val_accuracies = []
    val_f1s = []
    val_roc_aucs = []

    edge_index, _ = subgraph(test_nodes.to(device), edge_index, relabel_nodes=True)
    edge_index = edge_index.to(device)


    for run in range(10):
        seed = 0 + run
        torch.manual_seed(seed)
        np.random.seed(seed)
        
        if args.data == 'squirrel':
            x_feature = torch.Tensor(x_feature)
            
        X = x_feature[test_nodes].to(device)
        labels = data.y[test_nodes].to(device)
        X_combined = torch.zeros_like(x_feature).to(device)

        # Data split, following a 7:1:2 ratio
        train_ratio = 0.5
        val_ratio = 0.25
        test_ratio = 0.25
        num_nodes = X.shape[0]
        indices = list(range(num_nodes))
        
        # Split data based on random seed
        train_indices, temp_indices = train_test_split(indices, train_size=train_ratio, random_state=seed)
        val_indices, test_indices = train_test_split(temp_indices, test_size=test_ratio / (val_ratio + test_ratio), random_state=seed)

        # Convert to Tensor
        train_indices = torch.tensor(train_indices, device=device)
        val_indices = torch.tensor(val_indices, device=device)
        test_indices = torch.tensor(test_indices, device=device)


        # Initialize the model

        input_dim = X.shape[1]
        hidden_dim = args.hidden_size
        output_dim = len(torch.unique(labels))

        if args.model == 'MLP':
            model = MLP(input_dim, hidden_dim, output_dim).to(device)
        elif args.model == 'GCN':
            model = GCN(input_dim, hidden_dim, output_dim).to(device)
        elif args.model == 'GAT':
            model = GAT(input_dim, hidden_dim, output_dim, heads=args.heads).to(device)
        elif args.model == 'AP':
            model = AP(input_dim, hidden_dim, output_dim, args).to(device)
        else:
            raise ValueError("Unsupported model type. Choose from 'MLP', 'GCN', or 'GAT'.")

        # Loss function and optimizer
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=args.lr)
        arbloss = arbLoss(data.edge_index, x_all, trn_nodes, alpha=0.9, beta=0.9, device = device).to(device)



        # Early stopping parameters
        patience = 300
        epochs_without_improvement = 0

        num_epochs = args.epochs
        best_val_acc = 0
        best_val_f1 = 0
        best_val_roc_auc = 0
        best_test_acc = 0
        best_test_f1 = 0
        best_test_roc_auc = 0
        best_model_weights = None

        for epoch in range(num_epochs):
            model.train()
            optimizer.zero_grad()

            # Training phase
            if args.model == 'MLP': 
                out, re_x = model(X[train_indices])
                OUT, _ = model(X)
                _, train_predicted = torch.max(out, 1)
                # train_predicted = train_predicted[train_indices]
                # X_combined[trn_nodes] = x_feature[trn_nodes]  # Keep train features unchanged
                # X_combined[test_nodes] = re_x.detach()  # Place transformed test features
                criterion_loss = criterion(out, labels[train_indices])
                # arbloss_loss = arbloss.get_loss(X_combined)
                train_loss = criterion_loss #+ 0.001*arbloss_loss

                # print(arbloss_loss)
            else:  
                out = model(X, edge_index)
                #x_feature = torch.tensor(x_feature, requires_grad=True, device=device)
                train_loss = criterion(out[train_indices], labels[train_indices]) # arbloss.get_loss(x_loss)
                _, train_predicted = torch.max(out, 1)
                train_predicted = train_predicted[train_indices]
            train_loss.backward()
            optimizer.step()

            
            train_acc = accuracy_score(labels[train_indices].cpu(), train_predicted.cpu())
            train_f1 = f1_score(labels[train_indices].cpu(), train_predicted.cpu(), average='weighted')

            # Evaluate on the validation set

            model.eval()
            with torch.no_grad():
                if args.model == 'MLP':
                    val_out, _ = model(X[val_indices])
                else:
                    val_out = model(X, edge_index)
                    val_out = val_out[val_indices]
                _, val_predicted = torch.max(val_out, 1)
                val_acc = accuracy_score(labels[val_indices].cpu(), val_predicted.cpu())
                val_f1 = f1_score(labels[val_indices].cpu(), val_predicted.cpu(), average='weighted')
                val_prob = torch.softmax(val_out, dim=1).cpu().numpy()

                if args.data == "Minesweeper" or args.data == "Tolokers" or args.data == "Questions":
                    val_roc_auc = roc_auc_score(labels[val_indices].cpu().numpy(), val_prob[:, 1])
                else:
                    val_roc_auc = roc_auc_score(labels[val_indices].cpu().numpy(), val_prob, average='weighted', multi_class='ovr')

        # Evaluate on the testset
                if args.model == 'MLP':
                    test_out, _  = model(X[test_indices])
                    _, test_predicted = torch.max(test_out, 1)
                else:
                    test_out = model(X, edge_index)
                    test_out = test_out[test_indices]
                _, test_predicted = torch.max(test_out, 1)
                test_acc = accuracy_score(labels[test_indices].cpu(), test_predicted.cpu())
                test_f1 = f1_score(labels[test_indices].cpu(), test_predicted.cpu(), average='weighted')
                test_prob = torch.softmax(test_out, dim=1).cpu().numpy()

                if args.data == "Minesweeper" or args.data == "Tolokers" or args.data == "Questions":
                    test_roc_auc = roc_auc_score(labels[test_indices].cpu().numpy(), test_prob[:, 1])
                else:
                    test_roc_auc = roc_auc_score(labels[test_indices].cpu().numpy(), test_prob, average='weighted', multi_class='ovr')


            if val_roc_auc > best_val_roc_auc:
                best_val_acc = val_acc
                best_val_f1 = val_f1
                best_val_roc_auc = val_roc_auc
                best_test_acc = test_acc
                best_test_f1 = test_f1
                best_test_roc_auc = test_roc_auc
                best_model_weights = model.state_dict()
            else:
                epochs_without_improvement += 1
                # Early stopping
            # if epochs_without_improvement >= patience:
            #     print("Early stopping at epoch", epoch)
            #     break

        # Load the best model weights
        model.load_state_dict(best_model_weights)
        model.eval()

        with torch.no_grad():
            if args.model == 'MLP':
                final_val_out, _  = model(X[val_indices])
            else:
                final_val_out = model(X, edge_index)
                final_val_out = final_val_out[val_indices]
            _, final_val_predicted = torch.max(final_val_out, 1)
            final_val_acc = accuracy_score(labels[val_indices].cpu(), final_val_predicted.cpu())
            final_val_f1 = f1_score(labels[val_indices].cpu(), final_val_predicted.cpu(), average='weighted')

            if args.model == 'MLP':
                final_test_out, _  = model(X[test_indices])
            else:
                final_test_out = model(X, edge_index)
                final_test_out = final_test_out[test_indices]
            _, final_test_predicted = torch.max(final_test_out, 1)
            final_test_acc = accuracy_score(labels[test_indices].cpu(), final_test_predicted.cpu())
            final_test_f1 = f1_score(labels[test_indices].cpu(), final_test_predicted.cpu(), average='weighted')

            # ROC AUC
            final_val_prob = torch.softmax(final_val_out, dim=1).cpu().numpy()
            final_test_prob = torch.softmax(final_test_out, dim=1).cpu().numpy()

            if args.data == "Minesweeper" or args.data == "Tolokers" or args.data == "Questions":
                final_val_roc_auc = roc_auc_score(labels[val_indices].cpu().numpy(), final_val_prob[:, 1])
                final_test_roc_auc = roc_auc_score(labels[test_indices].cpu().numpy(), final_test_prob[:, 1])
            else:
                final_val_roc_auc = roc_auc_score(labels[val_indices].cpu().numpy(), final_val_prob, average='weighted', multi_class='ovr')
                final_test_roc_auc = roc_auc_score(labels[test_indices].cpu().numpy(), final_test_prob, average='weighted', multi_class='ovr')
                
            val_accuracies.append(final_val_acc)
            val_f1s.append(final_val_f1)
            val_roc_aucs.append(final_val_roc_auc)

            test_accuracies.append(final_test_acc)
            test_f1s.append(final_test_f1)
            test_roc_aucs.append(final_test_roc_auc)

            print(f'Run {run+1} - Final Validation Accuracy: {final_val_acc*100:.2f}, Final Validation F1: {final_val_f1*100:.2f}, Final Validation ROC AUC: {final_val_roc_auc*100:.2f}')
            print(f'Run {run+1} - Final Test Accuracy: {final_test_acc*100:.2f}, Final Test F1: {final_test_f1*100:.2f}, Final Test ROC AUC: {final_test_roc_auc*100:.2f}')

    mean_val_acc = np.mean(val_accuracies)
    std_val_acc = np.std(val_accuracies)
    mean_val_f1 = np.mean(val_f1s)
    std_val_f1 = np.std(val_f1s)
    mean_val_roc_auc = np.mean(val_roc_aucs)
    std_val_roc_auc = np.std(val_roc_aucs)

    mean_test_acc = np.mean(test_accuracies)
    std_test_acc = np.std(test_accuracies)
    mean_test_f1 = np.mean(test_f1s)
    std_test_f1 = np.std(test_f1s)
    mean_test_roc_auc = np.mean(test_roc_aucs)
    std_test_roc_auc = np.std(test_roc_aucs)

    print(f'\nAverage Validation Accuracy: {mean_val_acc*100:.2f} ± {std_val_acc*100:.2f}')
    print(f'Average Validation F1: {mean_val_f1*100:.2f} ± {std_val_f1*100:.2f}')
    print(f'Average Validation ROC AUC: {mean_val_roc_auc*100:.2f} ± {std_val_roc_auc*100:.2f}')
    print(f'Average Test Accuracy: {mean_test_acc*100:.2f} ± {std_test_acc*100:.2f}')
    print(f'Average Test F1: {mean_test_f1*100:.2f} ± {std_test_f1*100:.2f}')
    print(f'Average Test ROC AUC: {mean_test_roc_auc*100:.2f} ± {std_test_roc_auc*100:.2f}')

    torch.save(OUT, os.path.join(output_dir, 'oout.pt'))




    MSE = Restructure[0]
    RMSE = Restructure[1]
    MAE = Restructure[2]
    acc = final_test_acc
    f1 = final_test_f1
    auc = final_test_roc_auc
 
    result = [MSE, RMSE, MAE, acc, f1, auc]
    with open('./result/'+args.data+'results_241126.csv', 'a', newline='') as csvfile:
        csv_writer = csv.writer(csvfile)
        csv_writer.writerow(result)

if __name__ == '__main__':
    start = time.time()
    main()
    end = time.time()
    print(end-start)